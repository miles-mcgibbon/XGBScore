{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "recognized-german",
   "metadata": {},
   "source": [
    "2021-02-12\n",
    "\n",
    "# **XGBScore: A Gradient Boosted Decision Tree Scoring Function for Structure Based Virtual Screening**\n",
    "\n",
    "This is my lab book for my dissertation project. It will contain my daily work towards the project, and *in silico* experiments performed in python 3 code cells for debugging and evaluation before being turned into separate scripts for publication and analysis.\n",
    "\n",
    "### **Table of Contents**\n",
    "- Database Scraping\n",
    "- Dataset Assembly\n",
    "- Dataset Cleaning\n",
    "- Feature Engineering\n",
    "- Adding Decoys\n",
    "- Splitting the Data\n",
    "- Feature Importance/Dimensionality Reduction\n",
    "- Training the Model\n",
    "- Optimising the Model\n",
    "- Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "turned-amount",
   "metadata": {},
   "source": [
    "# **Database Scraping**\n",
    "\n",
    "While there will likely be lots of redundancy, four databases are being scraped for data for the training and test sets. These are:\n",
    "- Binding MOAD\n",
    "- PDBBind\n",
    "- Binding DB\n",
    "- DUD-E\n",
    "\n",
    "Generally, the approach will be to download the raw data into a folder called 'Dissertation/Data/Raw/{database_name}/Compressed', e.g. for Binding_MOAD: 'Dissertation/Data/Raw/Binding_MOAD/Compressed'. Then, the files will be extracted into 'Dissertation/Data/Raw/Binding_MOAD/Extracted'.\n",
    "\n",
    "## **Binding MOAD**\n",
    "\n",
    "Binding MOAD offers several different datasets. I have downloaded the structures (biounits) and binding data for the [**Non-redundant dataset only with binding data**](https://bindingmoad.org/Home/download). This dataset contains all complexes which have known binding data, and includes one 'leader' from each family with similar binding to prevent very similar data clogging the dataset. \n",
    "\n",
    "It has been downloaded to 'Dissertation/Data/Raw/Binding_MOAD/Compressed' and extracted to 'Dissertation/Data/Raw/Binding_MOAD/Extracted'. I will look at downloading the larger set once the pipeline for raw data to clean training and test datasets has been put together. \n",
    "\n",
    "\n",
    "The Binding MOAD files are stored as 'Biounit' files or .bio files, which are just partial structure PDB files and can be treated as shuch by changing the extension from '.bio1' to '.pdb'. We should download the original PDB files to a third folder 'Dissertation/Data/Raw/Binding_MOAD/original_PDB_files' just in case they are more useful. The code below downloads them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-trader",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_target_url(filepath):\n",
    "    \n",
    "    # get the pdb code from the filename\n",
    "    target_name = filepath.split('.')[0]\n",
    "    \n",
    "    # set the url string using the pdb code as where to download the pdb file\n",
    "    target_url = f'https://files.rcsb.org/download/{target_name}.pdb'\n",
    "    return target_url, target_name\n",
    "\n",
    "def save_target_file(url):\n",
    "    \n",
    "    # get the file url and the target name\n",
    "    url, target_name = create_target_url(url)\n",
    "    \n",
    "    # change this as to where you need to save the pdb files\n",
    "    target_path = f'/home/milesm/Dissertation/Data/Raw/Binding_MOAD/original_PDB_files/{target_name}.pdb'\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # ping the pdb and download the file if the url exists\n",
    "    if response.status_code == 200:\n",
    "        with open(target_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "            file.close()\n",
    "    else:\n",
    "        print(f'Whoops! Somethings wrong: Response {response.status_code}')\n",
    "\n",
    "\n",
    "# get the list of all the Binding_MOAD extracted protein-ligand complex biounit files\n",
    "filepaths = os.listdir('/home/milesm/Dissertation/Data/Raw/Binding_MOAD/Extracted/BindingMOAD_2020')\n",
    "\n",
    "# download the pdb files for all the Binding_MOAD extracted protein-ligand complex biounit files\n",
    "with tqdm(total=len(filepaths)) as pbar:\n",
    "    for target_file in filepaths:\n",
    "        save_target_file(target_file)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oriented-mainstream",
   "metadata": {},
   "source": [
    "## PDBBind\n",
    "\n",
    "I have downloaded the ['Protein-ligand complexes: The refined set'](http://www.pdbbind.org.cn/download.php) to the standard directory 'Dissertation/Data/Raw/PDBBind/Compressed', and extracted it to 'Dissertation/Data/Raw/PDBBind/Extracted'. No more needed to be done to the PDBBind dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "manual-innocent",
   "metadata": {},
   "source": [
    "## Binding DB\n",
    "\n",
    "Cannot see a way of differentiating only those complexes which have crystal structures attached to them. At this stage is is not being included in the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-columbia",
   "metadata": {},
   "source": [
    "## DUD-E\n",
    "\n",
    "For some reason, when I try to download the whole dataset at once, the server is throwing a 503/overload saying I've exceeded the maximum number of simultaneous downloads, despite it only being one tarball file. I think we can scrape the files one by one fairly quickly as there are only 102 of them. The files downloaded will be [these ones, the compressed file of the receptor and all actives and decoys in the individual target directory, like aa2ar.tar.gz](http://dude.docking.org/targets/aa2ar). Each file has a standard base url of 'dude.docking.org/targets/{target}/{target.tar.gz}'. So if we scrape all the hrefs from the main index page, we can just populate a list and ping them one at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applicable-removal",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# set the index url\n",
    "index_url = 'http://dude.docking.org/subsets/all'\n",
    "\n",
    "def create_target_url(url):\n",
    "    \n",
    "    # this creates the url string where the file should logically be stored on DUD-E\n",
    "    position = url.find('/targets/') + len('/targets/')\n",
    "    target_name = url[position:]\n",
    "    target_url = f'http://dude.docking.org/targets/{target_name}/{target_name}.tar.gz'\n",
    "    \n",
    "    # returns the url and the target protein name for use as the master folder name\n",
    "    return target_url, target_name\n",
    "\n",
    "def save_target_file(url):\n",
    "    \n",
    "    # get the url and the name for the folder to store the complexes in\n",
    "    url, target_name = create_target_url(url)\n",
    "    \n",
    "    # change this path to where you want the files to be downloaded to\n",
    "    target_path = f'/home/milesm/Dissertation/Data/Raw/DUD-E/Compressed/{target_name}.tar.gz'\n",
    "    \n",
    "    # ping the url and check it exists\n",
    "    response = requests.get(url, verify=False)\n",
    "    if response.status_code == 200:\n",
    "        \n",
    "        # save the compressed archive file with the decoys, actives and receptor in\n",
    "        with open(target_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "            file.close()\n",
    "    else:\n",
    "        print(f'Whoops! Somethings wrong: Response {response.status_code}')\n",
    "\n",
    "# check the DUD-E homepage for the HTML and make it readable with bs4\n",
    "index_page = requests.get(index_url)\n",
    "html_content = index_page.text\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# get urls of all the target proteins avaliable\n",
    "file_urls = [url['href'] for url in soup.find_all('a') if '/targets/' in str(url)]\n",
    "\n",
    "# for each target protein, download all the associated files\n",
    "with tqdm(total=len(file_urls)) as pbar:\n",
    "    for target_file in file_urls:\n",
    "        save_target_file(target_file)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-syndication",
   "metadata": {},
   "source": [
    "2021-02-13\n",
    "\n",
    "DUD-E files have been downloaded in a separate tar.gz file for each target protein. These all need extracting, which can be performed by the script below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "undefined-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# simple function to extract a file\n",
    "def extract_file(fname):\n",
    "    tar = tarfile.open(fname, \"r:gz\")\n",
    "    tar.extractall()\n",
    "    tar.close()\n",
    "\n",
    "# make a list of all the filepaths of the compressed protein target files in the folder\n",
    "files = [('/home/milesm/Dissertation/Data/Raw/DUD-E/Compressed/' + file) for file in os.listdir('/home/milesm/Dissertation/Data/Raw/DUD-E/Compressed/')]\n",
    "\n",
    "# extract all the files\n",
    "with tqdm(total=len(files)) as pbar:\n",
    "    for file in files:\n",
    "        extract_file(file)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-spectacular",
   "metadata": {},
   "source": [
    "These can then be sorted by filetype, and the extracted folders copied and pasted into the 'Dissertation/Data/Raw/DUD-E/Extracted' folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-basin",
   "metadata": {},
   "source": [
    "# **Data Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-brooklyn",
   "metadata": {},
   "source": [
    "2021-02-18\n",
    "\n",
    "## Software Installations\n",
    "\n",
    "All the software outlined below is needed for the steps of producing a csv file of features from each protein-ligand complex and decoys from DUD-E\n",
    "\n",
    "#### **PyMOL 2.4**\n",
    "\n",
    "Downloaded the tar.bz2 file from [PyMOL](https://pymol.org/2/), unpacked it and ran pymol using: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "cleared-assurance",
   "metadata": {},
   "source": [
    "cd pymol; ./pymol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "standing-angle",
   "metadata": {},
   "source": [
    "#### **BINANA**\n",
    "\n",
    "This is just a python2 script, and can be downloaded from [Professor Jacob Durrants Gitlab](https://durrantlab.pitt.edu/binana/), extracted and executed when needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smoking-algebra",
   "metadata": {},
   "source": [
    "#### **Autodock and Autodocktools via MGLTools 1.5.6**\n",
    "\n",
    "Downloaded the x64 GUI linux installer from the official source and executed it.\n",
    "\n",
    "#### **Openbabel 3.1.1**\n",
    "\n",
    "I have had to build openbabel several times from source with different flags for cmake before getting it working - what worked was installing:\n",
    "- libopenbabel-dev\n",
    "- libopenbabel4v5\n",
    "- openbabel-gui\n",
    "\n",
    "Using the command:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "vocational-income",
   "metadata": {},
   "source": [
    "sudo apt-get install libopenbabel-dev libopenbabel4v5 openbabel-gui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-smell",
   "metadata": {},
   "source": [
    "And then cloning openbabel from github and building from source using the instructions from the documentation with specific flags for the python bindings when using cmake:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "moving-microphone",
   "metadata": {},
   "source": [
    "cmake -DPYTHON_BINDINGS=ON -DRUN_SWIG=ON -DPYTHON_EXECUTABLE=/usr/bin/python3 ../openbabel-master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-calvin",
   "metadata": {},
   "source": [
    "#### **Python Libraries**\n",
    "\n",
    "All python libraries except openbabel were pip installed with \"python3 -m pip install xxx\"\n",
    "- BioPython\n",
    "- Open Drug Discovery Toolkit (ODDT)\n",
    "- Pandas\n",
    "- Numpy\n",
    "- Biopandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-coordinator",
   "metadata": {},
   "source": [
    "2021-02-20\n",
    "\n",
    "## **General Approach**\n",
    "\n",
    "The general approach for each cleaning each dataset and the end goals of the data cleaning can be seen below in Figure 1. The general ideal will be to clean data created in the last step from the 'Dissertation/Data/Raw/{database_name}/Extracted' directory into a standard format of:\n",
    "- A foldername with the pdb code containing:\n",
    "    - A ligand.pdb file\n",
    "    - A receptor.pdb file\n",
    "\n",
    "These folders will be stored in a new folder: 'Dissertation/Data/Parsed/{database_name}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-claim",
   "metadata": {},
   "source": [
    "<img src='Images/data_cleaning.png' align='center'/>\n",
    "\n",
    "*Figure 1 - Flowchart of planned data cleaning process*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bigger-strengthening",
   "metadata": {},
   "source": [
    "2021-02-21\n",
    "\n",
    "## **DUD-E Data Cleaning**\n",
    "\n",
    "### **Splitting active and decoy ligand multimol files**\n",
    "\n",
    "The DUD-E ligands and actives are stored in multimol .mol2 files, which need to be split into separate .mol2 files for GWOVina docking. This has been done with a custom python script which has been pushed to the github as 'split_ligands.py'. See the code comments for how it works. This script produces parsed DUD-E data in the form:\n",
    "\n",
    "- Foldername is protein target:\n",
    "    - 'actives' folder contains one mol2 file for each active ligand\n",
    "    - 'decoys' folder contains one mol2 file for each decoy\n",
    "    - crystal_ligand.mol2 file is a docked ligand\n",
    "    - receptor,pdb is the receptor pdb file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-reproduction",
   "metadata": {},
   "source": [
    "### **Docking active and decoy ligand mol2 files to target protein with GWOVina**\n",
    "\n",
    "The supplied actives and decoys do not appear to be docked for the DUD-E database. Therefore, each one needs docking with GWOVina before being converted to a PDBQT file with Autodocktools. This script is a work in progress, but it will automate the docking process and create a new subset of DUD-E data in 'Dissertation/Data/Parsed/DUD-E/Docked' with a folder for each protein-ligand and protein-decoy complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recognized-float",
   "metadata": {},
   "source": [
    "2021-02-22\n",
    "\n",
    "## **PDBBind Data Cleaning**\n",
    "\n",
    "The PDBBind Database ships in the ideal format for this project, and so no data cleaning needs to be undertaken for this dataset. It has been copied directly from the 'Dissertation/Data/Raw/PDBBind/Extracted' to the 'Dissertation/Data/Parsed/PDBBind' folder.\n",
    "\n",
    "## **Binding MOAD Data Cleaning**\n",
    "\n",
    "### **Splitting Protein-Ligand Complexes**\n",
    "\n",
    "The raw extracted Binding MOAD dataset just contains .pdb complex files, which need to be separated into a 'protein.pdb' file and a 'ligand.pdb' file. In .pdb files, amino acids/residues are stored as type 'ATOM', whereas non-amino acid residues are stored as type 'HETATM'. This 'HETATM' type includes sulfates, zincs and waters that are likely not ligands, as well as cofactors. \n",
    "\n",
    "Therefore, I have written a python script called 'split_complex.py' and pushed it to the project github. This looks for all the different residues where all the atoms are classed as 'HETATM', and then counts how many atoms are present in the residue, and keeps the longest 'HETATM' residue or ligand. This way, the chemical ligand is kept, as waters, zincs and sulfates have very few atoms compared to standard ligands. **This approach misses peptide ligands and this will need to be addressed ASAP. It also makes duplicate folders in the instance of some ligands due to multiple biounit files being present and one throwing an error**. The script produces a 'protein.pdb' and a 'ligand.pdb' file in a folder named as the complex pdb code in the master folder 'Dissertation/Data/Parsed/Binding_MOAD/'\n",
    "\n",
    "To resolve the above errors, I have written a Binding MOAD specific splitter named 'split_MOAD_complex.py'. This loads the binding data csv from Binding MOAD, and uses this to find the ID for the active ligand in the complex, then splits it and saves it separately from the protein, as well as stripping all the other HETATM types such as sulfates and waters. This script produces a folder for each biounit file called '/home/milesm/Dissertation/Data/Parsed/Binding_MOAD/{pdb_code}-X', where X is the biounit extension (e.g. .bio1 would be 1). Folders with only the protein file in are produced when the ligand was a peptide, or when no ligand was present in the particular biounit file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-administration",
   "metadata": {},
   "source": [
    "2021-03-01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-petite",
   "metadata": {},
   "source": [
    "# **Removing Duplicate Data Entries**\n",
    "\n",
    "Since PDBBind ships in the ideal format, and DUD-E actives are not docked, the order of preference for keeping structures in the case of duplicates is as follows:\n",
    "\n",
    "1. PDBBind Structures\n",
    "2. Binding MOAD Structures\n",
    "3. DUD-E Active Structures\n",
    "\n",
    "There is a difficult step involved in this part, as **DUD-E protein-ligand complexes are not labelled by PDB code but by CHEMBLID and Protein Target name**. Therefore, I will need to think of a way to remove duplicates after these have been docked.\n",
    "\n",
    "For PDBBind vs Binding MOAD, the duplicates can be stripped and the full non-redundant set copied  using some very simple python3 code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faced-proof",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "pdbbind_structures = os.listdir('/home/milesm/Dissertation/Data/Parsed/PDBBind')\n",
    "\n",
    "binding_MOAD_structures = os.listdir('/home/milesm/Dissertation/Data/Parsed/Binding_MOAD')\n",
    "\n",
    "pdbbind_clean_structures = [structure.upper() for structure in pdbbind_structures]\n",
    "binding_MOAD_clean_structures = list(set(list([structure.split('_')[0].upper() for structure in binding_MOAD_structures])))\n",
    "\n",
    "MOAD_exclusives = list()\n",
    "\n",
    "for structure in binding_MOAD_clean_structures:\n",
    "    if structure not in pdbbind_clean_structures:\n",
    "        MOAD_exclusives.append(structure)\n",
    "\n",
    "print(f'{len(MOAD_exclusives)} structures exclusive to MOAD')\n",
    "\n",
    "os.mkdir('/home/milesm/Dissertation/Data/Parsed/Non_redundant')\n",
    "os.mkdir('/home/milesm/Dissertation/Data/Parsed/Non_redundant/PDBBind')\n",
    "os.mkdir('/home/milesm/Dissertation/Data/Parsed/Non_redundant/Binding_MOAD')\n",
    "\n",
    "pdbbind_structures_filepaths = [f'/home/milesm/Dissertation/Data/Parsed/PDBBind/{file}' for file in pdbbind_structures]\n",
    "\n",
    "MOAD_exclusives_filenames = list()\n",
    "for pdb_code in MOAD_exclusives:\n",
    "    for filename in binding_MOAD_structures:\n",
    "        pdb_code_upper = pdb_code.upper()\n",
    "        filename_upper = filename.upper()\n",
    "        if pdb_code_upper in filename_upper:\n",
    "            MOAD_exclusives_filenames.append(filename)\n",
    "            \n",
    "print(f'Found {len(MOAD_exclusives_filenames)} exclusive MOAD files for copying')\n",
    "\n",
    "binding_MOAD_exclusive_structures_filepaths = [f'/home/milesm/Dissertation/Data/Parsed/Binding_MOAD/{file}' for file in MOAD_exclusives_filenames]\n",
    "\n",
    "with tqdm(total=len(pdbbind_structures)) as pbar:\n",
    "    for filepath, file in zip(pdbbind_structures_filepaths, pdbbind_structures):\n",
    "        shutil.copytree(filepath, f'/home/milesm/Dissertation/Data/Parsed/Non_redundant/PDBBind/{file}')\n",
    "        pbar.update(1)\n",
    "\n",
    "\n",
    "with tqdm(total=len(MOAD_exclusives_filenames)) as pbar:\n",
    "    for filepath, file in zip(binding_MOAD_exclusive_structures_filepaths, MOAD_exclusives_filenames):\n",
    "        shutil.copytree(filepath, f'/home/milesm/Dissertation/Data/Parsed/Non_redundant/Binding_MOAD/{file}')\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "canadian-arrangement",
   "metadata": {},
   "source": [
    "## **Conversion from .pdb to .pdbqt using Autodocktools**\n",
    "\n",
    "All the datasets should now be in the same ligand.pdb and protein.pdb format, with duplicates removed, in the 'Dissertation/Data/Parsed/Master' folder. The next step is to convert these pdb files into pdbqt files using autodocktools. This is a command line process. Autodocktools ships with two python scripts:\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "foreign-rebound",
   "metadata": {},
   "source": [
    "MGLTools-1.5.6/MGLToolsPckgs/AutoDockTools/Utilities24/prepare_ligand4.py\n",
    "MGLTools-1.5.6/MGLToolsPckgs/AutoDockTools/Utilities24/prepare_receptor4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-belize",
   "metadata": {},
   "source": [
    "These scripts convert pdb files to pdbqt files. Here, I will use the following commands to convert the pdb files to pdbqt files:\n",
    "\n",
    "For the ligand pdb file, this command adds hydrogens and gasteiger charges:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "logical-lucas",
   "metadata": {},
   "source": [
    "- prepare_ligand4.py -l ligand.pdb -A hydrogens -o ligand.pdbqt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-looking",
   "metadata": {},
   "source": [
    "For the protein pdb file, this command adds hydrogens and gasteiger charges, and removes any remaining waters:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fitted-layer",
   "metadata": {},
   "source": [
    "- prepare_receptor4.py -r protein.pdb -A hydrogens -o protein.pdbqt -U waters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "imperial-newcastle",
   "metadata": {},
   "source": [
    "I have written a python 3 script 'pdbqt_batch_converter.py' which takes all the protein and ligand pdb files in the 'Dissertation/Data/Parsed/Non-redundant' folders, converts them using the autodocktools command line commands above and then saves them in the 'Dissertation/Data/PDBQT/Non-redundant' folder. It has been pushed to the project github. **Currently, this script does not work for some Binding MOAD files, as they either have peptide ligands, or have two ligands in one binding pocket which are stored as one single ligand in the PDB file. This confuses autodock when it cannot find a bond between them, and it ditches/doesn't save half the ligand.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-manor",
   "metadata": {},
   "source": [
    "2021-03-02\n",
    "\n",
    "## **Adding Features for Each Complex**\n",
    "\n",
    "Once a master dataset of clean pdbqt files has been produced by 'pdbqt_batch_converter.py', each protein.pdbqt and ligand.pdbqt needs to be fed through 'binana.py' which calculates the following features:\n",
    "\n",
    "- Atom-type pair counts within 2.5 angstroms\n",
    "- Atom-type pair counts within 4.0 angstroms\n",
    "- Ligand atom types\n",
    "- Summed electrostatic energy by atom-type pair, in J/mol\n",
    "- Number of rotatable bonds in the ligand\n",
    "- Active-site flexibility\n",
    "- Hydrogen bonds\n",
    "- Hydrophobic contacts (C-C)\n",
    "- pi-pi stacking interactions\n",
    "- T-stacking (face-to-edge) interactions\n",
    "- Cation-pi interactions\n",
    "- Salt Bridges\n",
    "\n",
    "This script is used in the following way in the command line to add the above features to a protein-ligand complex:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dried-queens",
   "metadata": {},
   "source": [
    "python binana.py -receptor /path/to/receptor.pdbqt -ligand /path/to/ligand.pdbqt > output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recorded-prefix",
   "metadata": {},
   "source": [
    "This command outputs a text file of information about the protein-ligand complex comtaining markdown tables. These output text files will be stored in a separate directory, 'Dissertation/Data/BINANA/'. Since the information is not readily convertable to .csv format, I am writing the python 3 script 'parse_binana_output.py' to read in the output text file, find the information and output a csv file with a single row. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-participation",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
