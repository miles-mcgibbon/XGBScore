{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "overhead-justice",
   "metadata": {},
   "source": [
    "2021-12-02\n",
    "\n",
    "# **XGBScore: A Gradient Boosted Decision Tree Scoring Function for Structure Based Virtual Screening**\n",
    "\n",
    "This is my lab book for my dissertation project. It will contain my daily work towards the project, and *in silico* experiments performed in python 3 code cells for debugging and evaluation before being turned into separate scripts for publication and analysis.\n",
    "\n",
    "### **Table of Contents**\n",
    "- Database Scraping\n",
    "- Dataset Assembly\n",
    "- Dataset Cleaning\n",
    "- Feature Engineering\n",
    "- Adding Decoys\n",
    "- Splitting the Data\n",
    "- Feature Importance/Dimensionality Reduction\n",
    "- Training the Model\n",
    "- Optimising the Model\n",
    "- Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "national-maine",
   "metadata": {},
   "source": [
    "# Database Scraping\n",
    "\n",
    "While there will likely be lots of redundancy, four databases are being scraped for data for the training and test sets. These are:\n",
    "- Binding MOAD\n",
    "- PDBBind\n",
    "- Binding DB\n",
    "- DUD-E\n",
    "\n",
    "## Binding MOAD\n",
    "\n",
    "Binding MOAD offers several different datasets. I have downloaded the structures and binding data for the **Non-redundant dataset only with binding data**. This dataset contains all complexes which have known binding data, and includes one 'leader' from each family with similar binding to prevent very similar data clogging the dataset. These are stored in the Dissertation/Data/Raw/Binding_MOAD folder.\n",
    "\n",
    "## PDBBind\n",
    "\n",
    "PDBBind server seems to be down at the moment, so I will return to this dataset later.\n",
    "\n",
    "## Binding DB\n",
    "\n",
    "Cannot see a way of differentiating only those complexes which have crystal structures attached to them. Let's move on to DUD-E\n",
    "\n",
    "## DUD-E\n",
    "\n",
    "For some reason, when I try to download the whole dataset at once, the server is throwing a 503/overload saying I've exceeded the maximum number of simultaneous downloads, despite it only being one tarball file. I think we can scrape the files one by one fairly quickly as there are only 102 of them. Each file has a standard base url of 'dude.docking.org/targets/{target}/{target.tar.gz}'. So if we scrape all the hrefs from the main index page, we can just populate a list and ping them one at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-sight",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|â–ˆ         | 11/102 [09:57<55:56, 36.89s/it]  "
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "index_url = 'http://dude.docking.org/subsets/all'\n",
    "\n",
    "def create_target_url(url):\n",
    "    position = url.find('/targets/') + len('/targets/')\n",
    "    target_name = url[position:]\n",
    "    target_url = f'http://dude.docking.org/targets/{target_name}/{target_name}.tar.gz'\n",
    "    return target_url, target_name\n",
    "\n",
    "def save_target_file(url):\n",
    "    url, target_name = create_target_url(url)\n",
    "    target_path = f'/home/milesm/Dissertation/Data/Raw/DUD-E/{target_name}.tar.gz'\n",
    "    response = requests.get(url, verify=False)\n",
    "    if response.status_code == 200:\n",
    "        with open(target_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "            file.close()\n",
    "    else:\n",
    "        print(f'Whoops! Somethings wrong: Response {response.status_code}')\n",
    "\n",
    "index_page = requests.get(index_url)\n",
    "html_content = index_page.text\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "file_urls = [url['href'] for url in soup.find_all('a') if '/targets/' in str(url)]\n",
    "\n",
    "with tqdm(total=len(file_urls)) as pbar:\n",
    "    for target_file in file_urls:\n",
    "        save_target_file(target_file)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-metro",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
