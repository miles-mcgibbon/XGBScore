{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "favorite-skill",
   "metadata": {},
   "source": [
    "2021-02-12\n",
    "\n",
    "# **XGBScore: A Gradient Boosted Decision Tree Scoring Function for Structure Based Virtual Screening**\n",
    "\n",
    "This is my lab book for my dissertation project. It will contain my daily work towards the project, and *in silico* experiments performed in python 3 code cells for debugging and evaluation before being turned into separate scripts for publication and analysis.\n",
    "\n",
    "### **Table of Contents**\n",
    "- Database Scraping\n",
    "- Dataset Assembly\n",
    "- Dataset Cleaning\n",
    "- Feature Engineering\n",
    "- Adding Decoys\n",
    "- Splitting the Data\n",
    "- Feature Importance/Dimensionality Reduction\n",
    "- Training the Model\n",
    "- Optimising the Model\n",
    "- Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-banana",
   "metadata": {},
   "source": [
    "# **Database Scraping**\n",
    "\n",
    "While there will likely be lots of redundancy, four databases are being scraped for data for the training and test sets. These are:\n",
    "- Binding MOAD\n",
    "- PDBBind\n",
    "- Binding DB\n",
    "- DUD-E\n",
    "\n",
    "Generally, the approach will be to download the raw data into a folder called 'Dissertation/Data/Raw/{database_name}/Compressed', e.g. for Binding_MOAD: 'Dissertation/Data/Raw/Binding_MOAD/Compressed'. Then, the files will be extracted into 'Dissertation/Data/Raw/Binding_MOAD/Extracted'.\n",
    "\n",
    "## **Binding MOAD**\n",
    "\n",
    "Binding MOAD offers several different datasets. I have downloaded the structures (biounits) and binding data for the [**Non-redundant dataset only with binding data**](https://bindingmoad.org/Home/download). This dataset contains all complexes which have known binding data, and includes one 'leader' from each family with similar binding to prevent very similar data clogging the dataset. \n",
    "\n",
    "It has been downloaded to 'Dissertation/Data/Raw/Binding_MOAD/Compressed' and extracted to 'Dissertation/Data/Raw/Binding_MOAD/Extracted'. I will look at downloading the larger set once the pipeline for raw data to clean training and test datasets has been put together. \n",
    "\n",
    "\n",
    "The Binding MOAD files are stored as 'Biounit' files or .bio files, which are just partial structure PDB files and can be treated as shuch by changing the extension from '.bio1' to '.pdb'. We should download the original PDB files to a third folder 'Dissertation/Data/Raw/Binding_MOAD/original_PDB_files' just in case they are more useful. The code below downloads them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-impact",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_target_url(filepath):\n",
    "    \n",
    "    # get the pdb code from the filename\n",
    "    target_name = filepath.split('.')[0]\n",
    "    \n",
    "    # set the url string using the pdb code as where to download the pdb file\n",
    "    target_url = f'https://files.rcsb.org/download/{target_name}.pdb'\n",
    "    return target_url, target_name\n",
    "\n",
    "def save_target_file(url):\n",
    "    \n",
    "    # get the file url and the target name\n",
    "    url, target_name = create_target_url(url)\n",
    "    \n",
    "    # change this as to where you need to save the pdb files\n",
    "    target_path = f'/home/milesm/Dissertation/Data/Raw/Binding_MOAD/original_PDB_files/{target_name}.pdb'\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # ping the pdb and download the file if the url exists\n",
    "    if response.status_code == 200:\n",
    "        with open(target_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "            file.close()\n",
    "    else:\n",
    "        print(f'Whoops! Somethings wrong: Response {response.status_code}')\n",
    "\n",
    "\n",
    "# get the list of all the Binding_MOAD extracted protein-ligand complex biounit files\n",
    "filepaths = os.listdir('/home/milesm/Dissertation/Data/Raw/Binding_MOAD/Extracted/BindingMOAD_2020')\n",
    "\n",
    "# download the pdb files for all the Binding_MOAD extracted protein-ligand complex biounit files\n",
    "with tqdm(total=len(filepaths)) as pbar:\n",
    "    for target_file in filepaths:\n",
    "        save_target_file(target_file)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brave-guitar",
   "metadata": {},
   "source": [
    "## PDBBind\n",
    "\n",
    "I have downloaded the ['Protein-ligand complexes: The refined set'](http://www.pdbbind.org.cn/download.php) to the standard directory 'Dissertation/Data/Raw/PDBBind/Compressed', and extracted it to 'Dissertation/Data/Raw/PDBBind/Extracted'. No more needed to be done to the PDBBind dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "invisible-measurement",
   "metadata": {},
   "source": [
    "## Binding DB\n",
    "\n",
    "Cannot see a way of differentiating only those complexes which have crystal structures attached to them. At this stage is is not being included in the project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedicated-arrest",
   "metadata": {},
   "source": [
    "## DUD-E\n",
    "\n",
    "For some reason, when I try to download the whole dataset at once, the server is throwing a 503/overload saying I've exceeded the maximum number of simultaneous downloads, despite it only being one tarball file. I think we can scrape the files one by one fairly quickly as there are only 102 of them. The files downloaded will be [these ones, the compressed file of the receptor and all actives and decoys in the individual target directory, like aa2ar.tar.gz](http://dude.docking.org/targets/aa2ar). Each file has a standard base url of 'dude.docking.org/targets/{target}/{target.tar.gz}'. So if we scrape all the hrefs from the main index page, we can just populate a list and ping them one at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-neighbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# set the index url\n",
    "index_url = 'http://dude.docking.org/subsets/all'\n",
    "\n",
    "def create_target_url(url):\n",
    "    \n",
    "    # this creates the url string where the file should logically be stored on DUD-E\n",
    "    position = url.find('/targets/') + len('/targets/')\n",
    "    target_name = url[position:]\n",
    "    target_url = f'http://dude.docking.org/targets/{target_name}/{target_name}.tar.gz'\n",
    "    \n",
    "    # returns the url and the target protein name for use as the master folder name\n",
    "    return target_url, target_name\n",
    "\n",
    "def save_target_file(url):\n",
    "    \n",
    "    # get the url and the name for the folder to store the complexes in\n",
    "    url, target_name = create_target_url(url)\n",
    "    \n",
    "    # change this path to where you want the files to be downloaded to\n",
    "    target_path = f'/home/milesm/Dissertation/Data/Raw/DUD-E/Compressed/{target_name}.tar.gz'\n",
    "    \n",
    "    # ping the url and check it exists\n",
    "    response = requests.get(url, verify=False)\n",
    "    if response.status_code == 200:\n",
    "        \n",
    "        # save the compressed archive file with the decoys, actives and receptor in\n",
    "        with open(target_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "            file.close()\n",
    "    else:\n",
    "        print(f'Whoops! Somethings wrong: Response {response.status_code}')\n",
    "\n",
    "# check the DUD-E homepage for the HTML and make it readable with bs4\n",
    "index_page = requests.get(index_url)\n",
    "html_content = index_page.text\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# get urls of all the target proteins avaliable\n",
    "file_urls = [url['href'] for url in soup.find_all('a') if '/targets/' in str(url)]\n",
    "\n",
    "# for each target protein, download all the associated files\n",
    "with tqdm(total=len(file_urls)) as pbar:\n",
    "    for target_file in file_urls:\n",
    "        save_target_file(target_file)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "diverse-sleeve",
   "metadata": {},
   "source": [
    "2021-02-13\n",
    "\n",
    "DUD-E files have been downloaded in a separate tar.gz file for each target protein. These all need extracting, which can be performed by the script below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comic-quick",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# simple function to extract a file\n",
    "def extract_file(fname):\n",
    "    tar = tarfile.open(fname, \"r:gz\")\n",
    "    tar.extractall()\n",
    "    tar.close()\n",
    "\n",
    "# make a list of all the filepaths of the compressed protein target files in the folder\n",
    "files = [('/home/milesm/Dissertation/Data/Raw/DUD-E/Compressed/' + file) for file in os.listdir('/home/milesm/Dissertation/Data/Raw/DUD-E/Compressed/')]\n",
    "\n",
    "# extract all the files\n",
    "with tqdm(total=len(files)) as pbar:\n",
    "    for file in files:\n",
    "        extract_file(file)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "viral-architect",
   "metadata": {},
   "source": [
    "These can then be sorted by filetype, and the extracted folders copied and pasted into the 'Dissertation/Data/Raw/DUD-E/Extracted' folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-sound",
   "metadata": {},
   "source": [
    "# **Data Cleaning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-novel",
   "metadata": {},
   "source": [
    "2021-02-18\n",
    "\n",
    "## Software Installations\n",
    "\n",
    "All the software outlined below is needed for the steps of producing a csv file of features from each protein-ligand complex and decoys from DUD-E\n",
    "\n",
    "#### **PyMOL 2.4**\n",
    "\n",
    "Downloaded the tar.bz2 file from [PyMOL](https://pymol.org/2/), unpacked it and ran pymol using: "
   ]
  },
  {
   "cell_type": "raw",
   "id": "civilian-pleasure",
   "metadata": {},
   "source": [
    "cd pymol; ./pymol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honest-coach",
   "metadata": {},
   "source": [
    "#### **Autodock and Autodocktools via MGLTools 1.5.6**\n",
    "\n",
    "Downloaded the x64 GUI linux installer from the official source and executed it.\n",
    "\n",
    "#### **Openbabel 3.1.1**\n",
    "\n",
    "I have had to build openbabel several times from source with different flags for cmake before getting it working - what worked was installing:\n",
    "- libopenbabel-dev\n",
    "- libopenbabel4v5\n",
    "- openbabel-gui\n",
    "\n",
    "Using the command:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "blank-sally",
   "metadata": {},
   "source": [
    "sudo apt-get install libopenbabel-dev libopenbabel4v5 openbabel-gui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "toxic-blink",
   "metadata": {},
   "source": [
    "And then cloning openbabel from github and building from source using the instructions from the documentation with specific flags for the python bindings when using cmake:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "expected-ordering",
   "metadata": {},
   "source": [
    "cmake -DPYTHON_BINDINGS=ON -DRUN_SWIG=ON -DPYTHON_EXECUTABLE=/usr/bin/python3 ../openbabel-master"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-throw",
   "metadata": {},
   "source": [
    "#### **Python Libraries**\n",
    "\n",
    "All python libraries except openbabel were pip installed with \"python3 -m pip install xxx\"\n",
    "- BioPython\n",
    "- Open Drug Discovery Toolkit (ODDT)\n",
    "- Pandas\n",
    "- Numpy\n",
    "- Biopandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-scroll",
   "metadata": {},
   "source": [
    "2021-02-20\n",
    "\n",
    "## **General Approach**\n",
    "\n",
    "The general approach for each cleaning each dataset and the end goals of the data cleaning can be seen below in Figure 1. The general ideal will be to clean data created in the last step from the 'Dissertation/Data/Raw/{database_name}/Extracted' directory into a standard format of:\n",
    "- A foldername with the pdb code containing:\n",
    "    - A ligand.pdb file\n",
    "    - A receptor.pdb file\n",
    "\n",
    "These folders will be stored in a new folder: 'Dissertation/Data/Parsed/{database_name}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "professional-gallery",
   "metadata": {},
   "source": [
    "<img src='Images/data_cleaning_pipeline.png' align='center'/>\n",
    "\n",
    "*Figure 1 - Flowchart of planned data cleaning process*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-supplement",
   "metadata": {},
   "source": [
    "2021-02-21\n",
    "\n",
    "## **DUD-E Data Cleaning**\n",
    "\n",
    "### **Splitting active and decoy ligand multimol files**\n",
    "\n",
    "The DUD-E ligands and actives are stored in multimol .mol2 files, which need to be split into separate .mol2 files for GWOVina docking. This has been done with a custom python script which has been pushed to the github as 'split_ligands.py'. See the code comments for how it works. This script produces parsed DUD-E data in the form:\n",
    "\n",
    "- Foldername is protein target:\n",
    "    - 'actives' folder contains one mol2 file for each active ligand\n",
    "    - 'decoys' folder contains one mol2 file for each decoy\n",
    "    - crystal_ligand.mol2 file is a docked ligand\n",
    "    - receptor,pdb is the receptor pdb file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-halifax",
   "metadata": {},
   "source": [
    "### **Docking active and decoy ligand mol2 files to target protein with GWOVina**\n",
    "\n",
    "The supplied actives and decoys do not appear to be docked for the DUD-E database. Therefore, each one needs docking with GWOVina before being converted to a PDBQT file with Autodocktools. This script is a work in progress, but it will automate the docking process and create a new subset of DUD-E data in 'Dissertation/Data/Parsed/DUD-E/Docked' with a folder for each protein-ligand and protein-decoy complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "under-breathing",
   "metadata": {},
   "source": [
    "2021-02-22\n",
    "\n",
    "## **Binding MOAD Data Cleaning**\n",
    "\n",
    "### **Splitting Protein-Ligand Complexes**\n",
    "\n",
    "The raw extracted Binding MOAD dataset just contains .pdb complex files, which need to be separated into a 'protein.pdb' file and a 'ligand.pdb' file. In .pdb files, amino acids/residues are stored as type 'ATOM', whereas non-amino acid residues are stored as type 'HETATM'. This 'HETATM' type includes sulfates, zincs and waters that are likely not ligands, as well as cofactors. \n",
    "\n",
    "Therefore, I have written a python script called 'split_complex.py' and pushed it to the project github. This looks for all the different residues where all the atoms are classed as 'HETATM', and then counts how many atoms are present in the residue, and keeps the longest 'HETATM' residue or ligand. This way, the chemical ligand is kept, as waters, zincs and sulfates have very few atoms compared to standard ligands. **This approach misses peptide ligands and this will be addressed later**. The script produces a 'protein.pdb' and a 'ligand.pdb' file in a folder named as the complex pdb code, in the master folder 'Dissertation/Data/Parsed/Binding_MOAD/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-assault",
   "metadata": {},
   "source": [
    "2021-03-01\n",
    "\n",
    "The meeting with Dr Houston has been postponed by one day. I am going to look at the GWOVina and autodocktools command line usage and try to automate the docking and converstion process to .pdbqt files. It looks like the process goes:\n",
    "\n",
    "1. Convert to .pdbqt with autodocktools\n",
    "2. Dock with GWOVina (For DUD-E compounds and ligands)\n",
    "\n",
    "I will need to evaluate whether we need docking or not to actually score the ligands, or if they just need preparing with autodock.\n",
    "\n",
    "BINANA.py output has also been analysed, and outputs to a text file which I have written a script to parse and convert to a .csv file. I have got an error for some ligands when using autodocktools to convert to .pdbqt, I think it is to do with the fact those structures are dimers with two identical ligands docked.\n",
    "I have successfully sorted the dimer problem with two identical ligands by adding an error exception to the script. Autodocktools command line usage is working well for pdb to pdbqt conversion, but automation of the whole process will be much easier with openbabel imported as a part of oddt. Unfortunately, I have written a python script to use openbabel to do this but the resulting pdbqt file is problematic, and won't work in autodock or BINANA. I am going to do a side by side comparison of autodock output and openbabel output to see the differences and what might be causing them. If not, I will have to use rd-kit and build that from source to use as part of oddt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-tattoo",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
