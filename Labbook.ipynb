{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "herbal-walker",
   "metadata": {},
   "source": [
    "2021-02-12\n",
    "\n",
    "# **XGBScore: A Gradient Boosted Decision Tree Scoring Function for Structure Based Virtual Screening**\n",
    "\n",
    "This is my lab book for my dissertation project. It will contain my daily work towards the project, and *in silico* experiments performed in python 3 code cells for debugging and evaluation before being turned into separate scripts for publication and analysis.\n",
    "\n",
    "### **Table of Contents**\n",
    "- Database Scraping\n",
    "- Dataset Assembly\n",
    "- Dataset Cleaning\n",
    "- Feature Engineering\n",
    "- Adding Decoys\n",
    "- Splitting the Data\n",
    "- Feature Importance/Dimensionality Reduction\n",
    "- Training the Model\n",
    "- Optimising the Model\n",
    "- Evaluating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-school",
   "metadata": {},
   "source": [
    "# **Database Scraping**\n",
    "\n",
    "While there will likely be lots of redundancy, four databases are being scraped for data for the training and test sets. These are:\n",
    "- Binding MOAD\n",
    "- PDBBind\n",
    "- Binding DB\n",
    "- DUD-E\n",
    "\n",
    "## Binding MOAD\n",
    "\n",
    "Binding MOAD offers several different datasets. I have downloaded the structures and binding data for the **Non-redundant dataset only with binding data**. This dataset contains all complexes which have known binding data, and includes one 'leader' from each family with similar binding to prevent very similar data clogging the dataset. \n",
    "\n",
    "## PDBBind\n",
    "\n",
    "PDBBind server seems to be down at the moment, so I will return to this dataset later.\n",
    "\n",
    "## Binding DB\n",
    "\n",
    "Cannot see a way of differentiating only those complexes which have crystal structures attached to them. Let's move on to DUD-E.\n",
    "\n",
    "## DUD-E\n",
    "\n",
    "For some reason, when I try to download the whole dataset at once, the server is throwing a 503/overload saying I've exceeded the maximum number of simultaneous downloads, despite it only being one tarball file. I think we can scrape the files one by one fairly quickly as there are only 102 of them. Each file has a standard base url of 'dude.docking.org/targets/{target}/{target.tar.gz}'. So if we scrape all the hrefs from the main index page, we can just populate a list and ping them one at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-circular",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "index_url = 'http://dude.docking.org/subsets/all'\n",
    "\n",
    "def create_target_url(url):\n",
    "    position = url.find('/targets/') + len('/targets/')\n",
    "    target_name = url[position:]\n",
    "    target_url = f'http://dude.docking.org/targets/{target_name}/{target_name}.tar.gz'\n",
    "    return target_url, target_name\n",
    "\n",
    "def save_target_file(url):\n",
    "    url, target_name = create_target_url(url)\n",
    "    target_path = f'/home/milesm/Dissertation/Data/Raw/DUD-E/{target_name}.tar.gz'\n",
    "    response = requests.get(url, verify=False)\n",
    "    if response.status_code == 200:\n",
    "        with open(target_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "            file.close()\n",
    "    else:\n",
    "        print(f'Whoops! Somethings wrong: Response {response.status_code}')\n",
    "\n",
    "index_page = requests.get(index_url)\n",
    "html_content = index_page.text\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "file_urls = [url['href'] for url in soup.find_all('a') if '/targets/' in str(url)]\n",
    "\n",
    "with tqdm(total=len(file_urls)) as pbar:\n",
    "    for target_file in file_urls:\n",
    "        save_target_file(target_file)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applied-manor",
   "metadata": {},
   "source": [
    "2021-02-13\n",
    "\n",
    "DUD-E files have been downloaded in a separate tar.gz file for each one. These need extracting and sorting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-tender",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_file(fname):\n",
    "    tar = tarfile.open(fname, \"r:gz\")\n",
    "    tar.extractall()\n",
    "    tar.close()\n",
    "\n",
    "files = [('/home/milesm/Dissertation/Data/Raw/DUD-E/Compressed/' + file) for file in os.listdir('/home/milesm/Dissertation/Data/Raw/DUD-E/Compressed/')]\n",
    "\n",
    "with tqdm(total=len(files)) as pbar:\n",
    "    for file in files:\n",
    "        extract_file(file)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "certain-battle",
   "metadata": {},
   "source": [
    "The Binding MOAD files are stored as 'Biounit' files or .bio files, so I can't open them with PyMOL and have to use JMOLViewer. We should download the original PDB files as well just in case they are more useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-bangkok",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "def create_target_url(filepath):\n",
    "    target_name = filepath.split('.')[0]\n",
    "    target_url = f'https://files.rcsb.org/download/{target_name}.pdb'\n",
    "    return target_url, target_name\n",
    "\n",
    "def save_target_file(url):\n",
    "    url, target_name = create_target_url(url)\n",
    "    target_path = f'/home/milesm/Dissertation/Data/Raw/Binding_MOAD/original_PDB_files/{target_name}.pdb'\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(target_path, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "            file.close()\n",
    "    else:\n",
    "        print(f'Whoops! Somethings wrong: Response {response.status_code}')\n",
    "\n",
    "\n",
    "filepaths = os.listdir('/home/milesm/Dissertation/Data/Raw/Binding_MOAD/Extracted/BindingMOAD_2020')\n",
    "\n",
    "with tqdm(total=len(filepaths)) as pbar:\n",
    "    for target_file in filepaths:\n",
    "        save_target_file(target_file)\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-concert",
   "metadata": {},
   "source": [
    "# **Data Cleaning**\n",
    "\n",
    "Right now we have multiple files of separate ligands and receptors that we know either bind or don't bind. These will need (I think) docking and converting into pdbqt files for analysis by BINANA/Scoria. We will use ODDT and Autodock for this(?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noble-sustainability",
   "metadata": {},
   "source": [
    "2021-02-18\n",
    "\n",
    "I have had to build openbabel several times from source with different flags for cmake before getting it working - what worked was apt installing:\n",
    "- libopenbabel-dev\n",
    "- libopenbabel4v5\n",
    "- openbabel-gui\n",
    "\n",
    "And then cloning openbabel from github and building from source with specific flags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-filling",
   "metadata": {},
   "source": [
    "2021-02-20\n",
    "\n",
    "openbabels functionality was not immediately suitable for separating ligands from the protein-ligand complexes. I have written a different script using BioPython to split the Binding MOAD data into ligand and protein separate .pdb files and have pushed it to the project github. The overall data cleaning process has several steps (Fig. 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-movie",
   "metadata": {},
   "source": [
    "<img src='Images/data_cleaning_pipeline.png' align='center'/>\n",
    "\n",
    "*Figure 1 - Flowchart of planned data cleaning process*Assembly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-nevada",
   "metadata": {},
   "source": [
    "2021-02-21\n",
    "\n",
    "The DUD-E ligands and actives were stored in multimol .mol2 files, which needed to be split into separate .mol2 files for GWOVina docking. This has been done with a custom python script which has also been pushed to the github. I have a meeting with Dr. Houston tomorrow to discuss the workflow above and confirm it is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrapped-democrat",
   "metadata": {},
   "source": [
    "2021-02-22\n",
    "\n",
    "The meeting with Dr Houston has been postponed by one day. I am going to look at the GWOVina command line usage and try to automate the docking and converstion process to .pdbqt files. It looks like the process goes:\n",
    "\n",
    "1. Convert to .pdbqt with autodocktools\n",
    "2. Dock with GWOVina\n",
    "\n",
    "I will need to evaluate whether we need docking or not to actually score the ligands, or if they just need preparing with autodock."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
